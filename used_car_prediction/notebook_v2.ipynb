{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# --- Phase 1: Data Acquisition & Initial Exploration ---\n",
    "\n",
    "print(\"--- Phase 1: Data Acquisition & Initial Exploration ---\")\n",
    "\n",
    "# Load the dataset\n",
    "# Make sure 'car_dataset.csv' is in the same directory as your script/notebook,\n",
    "# or provide the full path to the file.\n",
    "try:\n",
    "    df = pd.read_csv('car_dataset.csv')\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'car_dataset.csv' not found. Please make sure the file is in the correct directory.\")\n",
    "    # In a real application, you might want to exit or raise a more specific error here.\n",
    "    exit()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\n--- First 5 rows of the dataset ---\")\n",
    "print(df.head())\n",
    "\n",
    "# Get basic information about the dataset (data types, non-null counts)\n",
    "print(\"\\n--- Dataset Info ---\")\n",
    "df.info()\n",
    "\n",
    "# Get descriptive statistics for numerical columns\n",
    "print(\"\\n--- Descriptive Statistics ---\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n--- Missing Values Count ---\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check unique values for categorical columns (important for understanding categories)\n",
    "# These are typical categorical columns for this dataset. Adjust if your dataset varies.\n",
    "typical_categorical_cols = ['Brand', 'Model', 'Fuel Type', 'Transmission', 'Ownership', 'Color']\n",
    "print(\"\\n--- Unique Values for Key Categorical Columns (Example Check) ---\")\n",
    "for col in typical_categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nUnique values in '{col}': {df[col].nunique()}\")\n",
    "        # print(df[col].value_counts()) # Uncomment to see counts of each unique value if needed\n",
    "\n",
    "# --- Phase 2: Data Preprocessing ---\n",
    "\n",
    "print(\"\\n--- Phase 2: Data Preprocessing ---\")\n",
    "\n",
    "# 1. Handle 'Unnamed: 0' column if it's an unnecessary index\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    print(\"Dropped 'Unnamed: 0' column.\")\n",
    "\n",
    "# 2. Impute Missing Values\n",
    "# Identify columns for imputation based on typical missing patterns in such datasets.\n",
    "# You should verify these columns based on your actual df.isnull().sum() output from Phase 1.\n",
    "numerical_cols_to_impute = ['Engine Size', 'Max Power', 'Torque'] # Common columns that might have missing values\n",
    "categorical_cols_to_impute = ['Fuel Type', 'Transmission'] # Example: if these had missing values\n",
    "\n",
    "# Impute numerical missing values with the median (robust to outliers)\n",
    "for col in numerical_cols_to_impute:\n",
    "    if col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            median_val = df[col].median()\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "            print(f\"Filled missing values in '{col}' with median: {median_val}\")\n",
    "\n",
    "# Impute categorical missing values with the mode (most frequent category)\n",
    "for col in categorical_cols_to_impute:\n",
    "    if col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            mode_val = df[col].mode()[0] # .mode() can return multiple if tied, take the first\n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"Filled missing values in '{col}' with mode: {mode_val}\")\n",
    "\n",
    "# Verify no more missing values (for the imputed columns)\n",
    "print(\"\\n--- Missing Values After Handling ---\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 3. Feature Engineering: Create 'Car_Age'\n",
    "# Assuming 'Year' column exists and represents manufacturing year\n",
    "if 'Year' in df.columns:\n",
    "    current_year = pd.Timestamp.now().year # Get the current year dynamically\n",
    "    df['Car_Age'] = current_year - df['Year']\n",
    "    df.drop('Year', axis=1, inplace=True) # Drop the original 'Year' column after creating 'Car_Age'\n",
    "    print(\"\\n'Car_Age' feature created and 'Year' column dropped.\")\n",
    "    print(df[['Car_Age', 'Price']].head())\n",
    "else:\n",
    "    print(\"\\n'Year' column not found for 'Car_Age' feature engineering.\")\n",
    "\n",
    "# Define categorical and numerical features for preprocessing pipeline\n",
    "# These lists should reflect the columns that remain after initial cleaning and feature engineering.\n",
    "# Ensure 'Price' is NOT in numerical_features as it's the target variable.\n",
    "categorical_features = ['Brand', 'Model', 'Fuel Type', 'Transmission', 'Ownership', 'Color']\n",
    "numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'Price' in numerical_features:\n",
    "    numerical_features.remove('Price') # Exclude target variable\n",
    "\n",
    "# Filter lists to include only columns actually present in the DataFrame after drops/renames\n",
    "categorical_features_present = [col for col in categorical_features if col in df.columns]\n",
    "numerical_features_present = [col for col in numerical_features if col in df.columns]\n",
    "\n",
    "# Create a ColumnTransformer for flexible preprocessing\n",
    "# This allows us to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features_present), # Apply StandardScaler to numerical features\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features_present) # One-Hot Encode categorical\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns not explicitly listed (e.g., 'Price' itself)\n",
    ")\n",
    "\n",
    "print(f\"\\nDefined numerical features for scaling: {numerical_features_present}\")\n",
    "print(f\"Defined categorical features for encoding: {categorical_features_present}\")\n",
    "print(\"\\n--- Data Preprocessing Complete (defined via ColumnTransformer) ---\")\n",
    "\n",
    "\n",
    "# --- Phase 3: Model Selection, Training & Evaluation ---\n",
    "\n",
    "print(\"\\n--- Phase 3: Model Selection, Training & Evaluation ---\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop('Price', axis=1) # All columns except 'Price'\n",
    "y = df['Price'] # The target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nData split: X_train:{X_train.shape}, X_test:{X_test.shape}, y_train:{y_train.shape}, y_test:{y_test.shape}\")\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# 1. Linear Regression Model\n",
    "print(\"\\n--- Training Linear Regression Model ---\")\n",
    "pipeline_lr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('regressor', LinearRegression())])\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "y_pred_lr = pipeline_lr.predict(X_test)\n",
    "mae_lr, rmse_lr, r2_lr = evaluate_model(\"Linear Regression\", y_test, y_pred_lr)\n",
    "\n",
    "# 2. Decision Tree Regressor Model\n",
    "print(\"\\n--- Training Decision Tree Regressor Model ---\")\n",
    "pipeline_dt = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('regressor', DecisionTreeRegressor(random_state=42))])\n",
    "pipeline_dt.fit(X_train, y_train)\n",
    "y_pred_dt = pipeline_dt.predict(X_test)\n",
    "mae_dt, rmse_dt, r2_dt = evaluate_model(\"Decision Tree Regressor\", y_test, y_pred_dt)\n",
    "\n",
    "# 3. Neural Network (MLPRegressor) Model\n",
    "print(\"\\n--- Training Neural Network (MLPRegressor) Model ---\")\n",
    "pipeline_mlp = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', MLPRegressor(hidden_layer_sizes=(100, 50), # Two layers with 100 and 50 neurons\n",
    "                                                          activation='relu', # ReLU activation\n",
    "                                                          solver='adam', # Adam optimizer\n",
    "                                                          max_iter=500, # Max iterations for convergence\n",
    "                                                          random_state=42,\n",
    "                                                          early_stopping=True, # Stop if validation score doesn't improve\n",
    "                                                          n_iter_no_change=20))]) # Number of epochs with no improvement to wait\n",
    "pipeline_mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = pipeline_mlp.predict(X_test)\n",
    "mae_mlp, rmse_mlp, r2_mlp = evaluate_model(\"Neural Network (MLPRegressor)\", y_test, y_pred_mlp)\n",
    "\n",
    "# 4. Hyperparameter Tuning for Decision Tree Regressor (Example using GridSearchCV)\n",
    "print(\"\\n--- Hyperparameter Tuning for Decision Tree Regressor using GridSearchCV ---\")\n",
    "param_grid_dt = {\n",
    "    'regressor__max_depth': [None, 10, 20, 30], # Maximum depth of the tree\n",
    "    'regressor__min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "    'regressor__min_samples_leaf': [1, 2, 4] # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "grid_search_dt = GridSearchCV(pipeline_dt, param_grid_dt, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters for Decision Tree: {grid_search_dt.best_params_}\")\n",
    "print(f\"Best cross-validation score (negative MSE): {grid_search_dt.best_score_:.2f}\")\n",
    "\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "y_pred_dt_tuned = best_dt_model.predict(X_test)\n",
    "mae_dt_tuned, rmse_dt_tuned, r2_dt_tuned = evaluate_model(\"Tuned Decision Tree Regressor\", y_test, y_pred_dt_tuned)\n",
    "\n",
    "\n",
    "# --- Phase 4: Model Analysis & Visualization ---\n",
    "\n",
    "print(\"\\n--- Phase 4: Model Analysis & Visualization ---\")\n",
    "\n",
    "# Choose your best performing model's predictions for visualization\n",
    "# For example, if Tuned Decision Tree was best:\n",
    "y_preds_best_model = y_pred_dt_tuned # Replace with predictions from your actual best model (e.g., y_pred_mlp, y_pred_lr)\n",
    "best_model_name = \"Tuned Decision Tree Regressor\" # Adjust accordingly based on your evaluation\n",
    "best_model_instance = best_dt_model # Store the actual best model instance\n",
    "\n",
    "print(f\"\\n--- Visualizing Performance for {best_model_name} ---\")\n",
    "\n",
    "# Actual vs. Predicted Prices Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=y_test, y=y_preds_best_model, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2) # Ideal line\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(f\"Actual vs. Predicted Prices ({best_model_name})\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Residuals Distribution Plot\n",
    "residuals = y_test - y_preds_best_model\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.histplot(residuals, kde=True, bins=50)\n",
    "plt.title(f\"Distribution of Residuals ({best_model_name})\")\n",
    "plt.xlabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predicted Prices Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=y_preds_best_model, y=residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Predicted Prices\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.title(f\"Residuals vs. Predicted Prices ({best_model_name})\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Importance (for Tree-based models like Decision Tree) ---\n",
    "# This section assumes your best model is a tree-based model and has feature_importances_\n",
    "# If your best model is Linear Regression or MLP, this part will not run directly.\n",
    "if hasattr(best_model_instance.named_steps['regressor'], 'feature_importances_'):\n",
    "    print(\"\\n--- Extracting Feature Importances ---\")\n",
    "    \n",
    "    # Get feature names from the preprocessor's transformers\n",
    "    # Ensure numerical_features_present and categorical_features_present are defined correctly above\n",
    "    numerical_ftrs_out = best_model_instance.named_steps['preprocessor'].named_transformers_['num'].get_feature_names_out(numerical_features_present)\n",
    "    categorical_ftrs_out = best_model_instance.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features_present)\n",
    "    \n",
    "    # Combine all feature names in the correct order as they come out of the preprocessor\n",
    "    all_feature_names_transformed = list(numerical_ftrs_out) + list(categorical_ftrs_out)\n",
    "\n",
    "    feature_importances = best_model_instance.named_steps['regressor'].feature_importances_\n",
    "    importance_df = pd.DataFrame({'Feature': all_feature_names_transformed, 'Importance': feature_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(\"\\n--- Top 10 Feature Importances ---\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\n",
    "    plt.title(\"Top 10 Feature Importances\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\nFeature importances are not directly available for {best_model_name} in this manner.\")\n",
    "    print(\"Consider advanced techniques like Permutation Importance or SHAP values for model-agnostic explanations if needed (advanced topic).\")\n",
    "\n",
    "print(\"\\n--- Project Code Execution Complete ---\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
